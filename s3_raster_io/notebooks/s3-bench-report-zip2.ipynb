{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "\n",
    "- Wofs LS5, full history query\n",
    "- Tile -9 -18, chunk (8,2)\n",
    "- Image Properties\n",
    "    - 4000x4000 single band uint8\n",
    "    - Chunk size 256x256\n",
    "    - DEFLATE lvl 9, no differencing\n",
    "- 1416 time slices\n",
    "- Access one chunk from each time slice\n",
    "- M5.xlarge instance 4 cores 16G ram\n",
    "- Chunk with largest compressed size was chosen\n",
    "- S3 bucket and EC2 both in Sydney region\n",
    "- \"Random\" 5-char prefix added to file names\n",
    "- http://dea-public-data.s3-website-ap-southeast-2.amazonaws.com/?prefix=bench-data/\n",
    "\n",
    "## Bespoke S3 TIFF reader\n",
    "\n",
    "Rather than using rasterio/GDAL we have custom reader that uses `botocore` to fetch data from S3 bucket, parses tif header, then reads requested chunk. Overall this equates to 2 requests per image when doing pixel drill type access.\n",
    "\n",
    "Reader limitations:\n",
    "\n",
    "- Tiled images only (COG only)\n",
    "- DEFLATE compression only\n",
    "- No predictor (expect to add, as it's common with 16bit images)\n",
    "- Only reads band 1 (easy to add support for multi-band images)\n",
    "- Doesn't interpret GEO referencing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from types import SimpleNamespace\n",
    "from utils import bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = sorted(glob.glob('./results/M5XL_ZIP2_-9_-18*_001.pickle'))\n",
    "dd = [pickle.load(open(file, 'rb')) for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling with more threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "best_idx = bench.plot_stats_results(dd, fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In depth stats for single threaded case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bench.gen_stats_report(dd[0]))\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "bench.plot_results(dd[0].stats, fig=fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "- Significantly lower open costs than gdal based solution\n",
    "- Open costs are slightly higher than read\n",
    "  - Open reads 4K chunk at the start of the file, which is less than the smallest data chunk, but is still slower\n",
    "  - Most likely first access costs more than second access\n",
    "- Scales well with more processing workers\n",
    "  - Limited by latency not throughput\n",
    "  - Likely limited by number of requests per second (just under 500 requests per second)\n",
    "  - For data with larger data chunks throughput will matter more I guess\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
